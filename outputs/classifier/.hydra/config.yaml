data_args:
  dataset_name: gsm8k_compressed
  data_path: data/gsm8k_compressed_train.json
  train_dev_test_split_ratio:
  - 0.8
  - 0.2
  - 0
  cache_dir: null
  max_seq_length: 2048
  pad_to_multiple_of: 8
  padding_side: left
  max_train_samples: null
  max_eval_samples: null
  max_predict_samples: null
model_args:
  model_name_or_path: unsloth/Meta-Llama-3.1-8B-Instruct
  config_name: null
  tokenizer_name: null
  compression_token: <COMP>
classifier_args:
  dropout: 0.1
  hidden_size: 4096
training_args:
  output_dir: outputs/classifier
  do_train: true
  do_eval: true
  do_predict: false
  seed: 42
  num_train_epochs: 3
  batch_size: 64
  micro_batch_size: 4
  learning_rate: 5.0e-05
  lr_scheduler_type: cosine
  resume_from_checkpoint: null
  warmup_percentage: 0.1
  evaluation_strategy: steps
  eval_steps: 100
  save_strategy: steps
  save_steps: 100
  save_total_limit: 3
  prediction_loss_only: false
  use_wandb: true
  wandb_usr: null
  wandb_project: hpml-dynamic-compression
  wandb_run_name: classifier-gsm8k-llama31
  wandb_watch: ''
  wandb_log_model: ''
