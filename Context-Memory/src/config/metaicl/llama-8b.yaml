# @package _global_
# Configuration for LLaMA-3.1-8B models
# Optimized for RTX 6000 (48GB) - reduced batch size compared to A100

defaults:
  - data

training:
  fp16: true
  fp16_full_eval: true

  # Reduced batch size for RTX 6000 (48GB vs A100 80GB)
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 128  # Effective batch = 128

  per_device_eval_batch_size: 2
  generation_max_length: 2560  # Longer for reasoning traces

  learning_rate: 3.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.0

  peft: true
  peft_type: lora
  lora_r: 8
  target_modules: q_proj,k_proj,v_proj,o_proj

  logging_steps: 50
  save_steps: 500
  eval_steps: 500

  max_steps: 2000

  # Memory optimization for smaller GPUs
  gradient_checkpointing: true
