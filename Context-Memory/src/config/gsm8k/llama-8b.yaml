# @package _global_
# Configuration for LLaMA-3.1-8B on GSM8K reasoning compression
# Optimized for RTX 6000 (48GB)

defaults:
  - data

training:
  fp16: true
  fp16_full_eval: true

  # Batch size for RTX 6000 (48GB)
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64  # Effective batch = 64

  per_device_eval_batch_size: 2
  generation_max_length: 2560

  learning_rate: 3.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  weight_decay: 0.0

  # LoRA configuration
  peft: true
  peft_type: lora
  lora_r: 8
  target_modules: q_proj,k_proj,v_proj,o_proj

  logging_steps: 25
  save_steps: 250
  eval_steps: 250

  max_steps: 1000

  # Memory optimization
  gradient_checkpointing: true

  # Compression settings (online = our method with dynamic <COMP>)
  comp:
    comp_type: online
    attn_type: concat_recur
    add_comp_token: true
    num_comp_tokens: 1
    cond_lora: true
    separate_embed: true
    relative_embedding: base
    sink: false
