# @package _global_
# Configuration for LLaMA-3.1-8B on GSM8K reasoning compression
# Optimized for RTX 6000 (48GB) with Flash Attention 2
#
# Override any config via CLI: python -m src.train ... training.max_steps=500

defaults:
  - data

model:
  # SDPA includes FlashAttention-2 in PyTorch 2.0+ (no extra package needed)
  attn_implementation: sdpa

training:
  # === Precision ===
  bf16: true
  bf16_full_eval: true
  fp16: false
  fp16_full_eval: false

  # === Batch Size ===
  # Optimized for RTX 6000 (48GB) with Flash Attention
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 16  # Effective batch = 64
  per_device_eval_batch_size: 4

  # === Training Duration ===
  # Use EITHER max_steps OR num_train_epochs (not both)
  # With 900 train samples and effective batch 64: ~14 steps/epoch
  max_steps: 300           # 300 steps = ~21 epochs
  # num_train_epochs: 20   # Alternative: specify epochs directly

  # === Learning Rate Schedule ===
  learning_rate: 1.0e-4      # Lower LR for stability (was 3e-4)
  lr_scheduler_type: cosine
  warmup_ratio: 0.1          # 10% warmup for stability (was 3%)
  weight_decay: 0.01
  max_grad_norm: 1.0         # Gradient clipping to prevent spikes

  # === Evaluation & Early Stopping ===
  do_eval: true
  predict_with_generate: false   # Disable generation during eval (loss-only, much faster)
  prediction_loss_only: true     # Don't accumulate logits during eval (saves memory)
  # NOTE: Generation during eval crashes due to attention mask bug in ccm_llama.py:441
  # The assertion `attention_mask.shape[-1] == seq_length_with_past` fails during KV-cache generation.
  # To fix: replace assertion with adaptive mask resizing. See git history for details.
  eval_strategy: steps           # Must match save_strategy for load_best_model_at_end
  eval_steps: 25                 # Eval every 25 steps
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false       # For loss, lower is better
  early_stopping_patience: 3     # Stop after N evals without improvement

  # === Checkpointing & Logging ===
  logging_steps: 10
  save_strategy: steps           # Must match eval_strategy
  save_steps: 25                 # Save every 25 steps (~15 min)
  save_total_limit: 5            # Keep last 5 checkpoints

  # === LoRA Configuration ===
  peft: true
  peft_type: lora
  lora_r: 16                 # Higher rank for more capacity (was 8)
  target_modules: q_proj,k_proj,v_proj,o_proj

  # === Memory Optimization ===
  gradient_checkpointing: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true

  # === Generation ===
  generation_max_length: 2560

  # === Compression Settings (CCM) ===
  comp:
    comp_type: online          # online = our method with dynamic <COMP>
    attn_type: concat_recur    # Recurrent attention with concatenation
    add_comp_token: true
    num_comp_tokens: 1
    cond_lora: true            # Conditional LoRA at COMP positions
    separate_embed: true
    relative_embedding: base
    sink: false
