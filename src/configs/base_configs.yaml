data_args:
  #  Dataset
  dataset_name: null
  dataset_config_name: null
  cache_dir: null
  max_train_samples: null
  max_eval_samples: null
  max_predict_samples: null

#  train_dev_test_split_ratio: [0.8, 0.1, 0.1]
  train_dev_test_split_ratio: [0.8, 0.2, 0]

  #  Tokenization
  max_seq_length: 1024
  pad_to_multiple_of: 8
  padding_side: "left"

model_args:   # configs for language model
  model_name_or_path: null
  config_name: null
  tokenizer_name: null
  compression_token: "<COMP>"

  # ----- lora hyperparameters -----
  # lora_weights: null
  #  lora_r: 16
  #  lora_alpha: 16
  #  lora_dropout: 0.05
  #  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "fc1", "fc2"]

classifier_args:
  dropout: 0.1

training_args:
  output_dir: outputs

  do_train: False
  do_eval: False
  do_predict: False

  seed: 42
  num_train_epochs: 3
  batch_size: 128
  micro_batch_size: 16
  learning_rate: 0.00005
  lr_scheduler_type: "cosine"
  resume_from_checkpoint: null
  warmup_percentage: 0.1

  evaluation_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 3

  prediction_loss_only: False

  # wandb
  use_wandb: True
  wandb_usr: null   # your wandb username
  wandb_project: "hpml-group-project-dynamic-compression"
  wandb_run_name: "test"
  wandb_watch: ""
  wandb_log_model: ""

hydra:
  run:
    dir: ${training_args.output_dir}